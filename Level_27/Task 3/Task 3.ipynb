{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-TUh6yIWEKh",
        "outputId": "76240259-973f-4740-c0f4-34b0a8780083"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\PRAVEE~1\\AppData\\Local\\Temp\\tmp7j6fr8bn\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\PRAVEE~1\\AppData\\Local\\Temp\\tmp7j6fr8bn\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\PRAVEE~1\\AppData\\Local\\Temp\\tmp7j6fr8bn'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  2374483124752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2374483125520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2374483126672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2374483125136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2374483128976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2374483129744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2374483129168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2374483128208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\PRAVEEN ANANTH\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Comparison:\n",
            "| Metric               | Original Model       | Quantized Model       |\n",
            "|----------------------|----------------------|------------------------|\n",
            "| Size (bytes)         | 1502296              | 130112                 |\n",
            "| Accuracy             | 0.9906               | 0.9906               |\n",
            "| Avg Inference Time   | 0.000075 s        | 0.000026 s        |\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = tf.keras.models.load_model('mnist_cnn.h5')\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "with open('mnist_cnn_quantized.tflite', 'wb') as f:\n",
        "    f.write(tflite_quantized_model)\n",
        "\n",
        "original_model_size = os.path.getsize('mnist_cnn.h5')\n",
        "quantized_model_size = os.path.getsize('mnist_cnn_quantized.tflite')\n",
        "\n",
        "(_, _), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred = model.predict(x_test, verbose=0)\n",
        "original_inference_time = (time.time() - start_time) / len(x_test)\n",
        "original_accuracy = accuracy_score(y_test, np.argmax(y_pred, axis=1))\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=\"mnist_cnn_quantized.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "predictions = []\n",
        "inference_times = []\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "    input_data = np.expand_dims(x_test[i], axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "    start = time.time()\n",
        "    interpreter.invoke()\n",
        "    inference_times.append(time.time() - start)\n",
        "\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predictions.append(np.argmax(output))\n",
        "\n",
        "quantized_accuracy = accuracy_score(y_test, predictions)\n",
        "quantized_inference_time = np.mean(inference_times)\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(\"| Metric               | Original Model       | Quantized Model       |\")\n",
        "print(\"|----------------------|----------------------|------------------------|\")\n",
        "print(f\"| Size (bytes)         | {original_model_size:<20} | {quantized_model_size:<22} |\")\n",
        "print(f\"| Accuracy             | {original_accuracy:.4f}               | {quantized_accuracy:.4f}               |\")\n",
        "print(f\"| Avg Inference Time   | {original_inference_time:.6f} s        | {quantized_inference_time:.6f} s        |\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWXXc0S0WttG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
